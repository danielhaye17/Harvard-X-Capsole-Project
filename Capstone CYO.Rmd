---
title: "Liver Disease Diagnosis CYO Project"
author: "Daniel Haye"
date: "6/3/2020"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, include=FALSE, echo=FALSE}


# Install all needed libraries if it is not present
if(!require(tidyverse)) install.packages("tidyverse") 
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(tidyr)) install.packages("tidyr")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(stringr)) install.packages("stringr")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(dplyr)) install.packages("dplyr")
if(!require(caret)) install.packages("caret")
if(!require(e1071)) install.packages("e1071")
if(!require(class)) install.packages("class")
if(!require(ROCR)) install.packages("ROCR")
if(!require(randomForest)) install.packages("randomForest")
if(!require(PRROC)) install.packages("PRROC")
if(!require(reshape2)) install.packages("reshape2")
if(!require(funModeling)) install.packages("funModeling")
if(!require(corrplot)) install.packages("corrplot")
if(!require(gridExtra))install.packages("gridExtra")
```

```{r, include=FALSE, echo=FALSE}
# Loading all needed libraries
library(dplyr)
library(tidyverse)
library(kableExtra)
library(tidyr)
library(ggplot2)
library(gbm)
library(caret)
library(xgboost)
library(e1071)
library(class)
library(ROCR)
library(randomForest)
library(PRROC)
library(reshape2)
library(funModeling)
library(corrplot)
library(gridExtra)
```

\newpage


# Introduction

The issue of Liver disease has been plaguing India for a number of years. It was even suggested that the increase in liver disease patients may have stemmed from the fact that persons are now increasing their  consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles, and drugs.

In general, the liver is evaluated with a group of test that all include: alanine transaminase, aspartate aminotransferase, alkaline phosphatase, albumin, total protein and bilirubin. 

While taking a blood test, if any of thes chemicals (bilirubin, Alkaline Phosphotase, Alamine Aminotransferase) are above normal range, it may result in liver disease. Additionally if the albumin or the total protien level in your bload is below the normal range, it may also mean that patient may have a liver disease. The normal range for each chemical are as follows:

* Total Bilirubin: 0.3–1.0 mg/dL
* Direct Bilirubin: 0–0.4 mg/dL
* Alkaline Phosphotase: 20 to 140 IU/L
* Alamine Aminotransferase: male 29-33 IU/L, female 19-25 IU/L 
* Aspartate Aminotransferase: male less than 50, female less than 45
* Total Protiens: 6-8.3 g/dL
* Albumin: 3.4-5.4 g/dL 


## Project Scope

This project is the final assignment in the HarvardX: PH125.9x Data Science: Capstone course. For this project, I had the option to choose any dataset to model a machine algorithm on. As such, I choose to use the Indian Liver Patient Records where the patient records were collected from North East of Andhra Pradesh, India.

This dataset will then be wrangled and an exploratory data analysis will be carried out in order to develop a machine learning algorithm. This algorithm will predict whether an individual is diagnosed with liver disease or not. After this, the model with the least error will be selected. Results will be explained followed by a conclussion with the closing remarks about the model.


## Aim of the project

The aim of this project is to train machine learning models to predict whether a person has liver disease or not to try an reduce the workload on the Indian doctors.  This will be done by assessing the chemical compounds (bilrubin, albumin, protiens, alkaline phosphatase) that are present in human body to diagnose the person. Data will be transformed and its dimension reduced to reveal patterns in the dataset and create a more robust analysis.

The model that optimizes the algorithm  will be chosen following the resulting accuracy, sensitivity and f1 score. These metrics will be defined later down in this project.

The machine learning models that we would like for this project should provide a good mix between a high accuracy level combined with a high sensitivity level. Here, accuracy indicates the number of outcomes that was currectly predicted by the model while sensitivity (True Positive Rate) indicates what proportion of people ‘with liver disease’ were correctly classified. Additionally, F1 score can be seen as the harmonic mean of precision and sensitivity.



## Dataset Summary

This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The "Dataset" column is a class label used to divide groups into liver patient (liver disease) or not (no disease).

*Source*:[www.kaggle.com](https://www.kaggle.com/uciml/indian-liver-patient-records?select=indian_liver_patient.csv)

\newpage 
# Knowing the Data 

## Data Exploration
To perform an analysis on a data, you must first know the dataset. As such, this section will give you a brief synapsis of the dataset.

This dataset consists of 583 observations with 11 variables. The collection of data spanned between 70 different ages where 441 were males and 142 were females. This shows that there is an imbalance between the number of males and females in the dataset. The top 5 ages represented in the data were: 60, 45, 50, 38 then 42.

Additionally, the variable dataset represents if the person is a liver disease patient or not. Here, 1 represent a patient with liver disease while 2 represent not having the disease. The dataset has only 4 NAs which are all in the “Albumin_and_Globulin_Ratio” variable.


**pdata dataset**
```{r }
## Loading the dataset from my github profile

pdata <- read.csv("https://raw.githubusercontent.com/danielhaye17/Harvard-X-Capsole-Project/master/datasets_indian_liver_patient.csv")



#Finding the amount of observations and variables in the datset
dim(pdata)

#Viewing the datatypes of the variables in the dataset
str(pdata)

#Summarizing the data
summary(pdata)

#Finding how many different ages are in the dataset
n_distinct(pdata$Age)

#Finding the number of males and females in the dataset
pdata%>%group_by(Gender)%>%summarise(count=n())%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)

#Finding the top 5 ages represented in the dataset
pdata%>%group_by(Age)%>%summarise(count=n())%>%
  arrange(desc(count))%>%head(n=5)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)

#Checking to see the number ofliver disease patients
pdata%>%group_by(Dataset)%>%summarise(count=n())%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)

#Finding out if there any data with N/As
sapply(pdata, function(g){
  sum(is.na(g))
})%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)

#Finding the mean of the Albumin_and_Globulin_Ratio variable
mean(pdata$Albumin_and_Globulin_Ratio,na.rm = TRUE)

#Viewing the NAs in the Albumin_and_Globulin_Ratio variable
pdata%>%filter(is.na(Albumin_and_Globulin_Ratio))%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
```


In general, the pdata dataset is a data frame which consist of eleven variables:

1. **Age**: An integer datatype that contains the age of each person.
2. **Gender**: A factor with two levels, Male and Female
3. **Total_Bilirubin**: A number datatype that contains the total number of billirubins for each person
4. **Direct_Bilirubin**: A number datatype that contains the number of direct billirubins for each person
5. **Alkaline_Phosphotase**: An integer that contains the number of Alkaline Phosphotase for each person
6. **Alamine_Aminotransferase**: An integer that contains the number of Alamine Aminotransferase for each person
7. **Aspartate_Aminotransferase**: An integer that contains the number of Aspartate Aminotransferase for each person
8. **Total_Protiens**: A number datatype that contains the total number of protiens for each person
9. **Albumin**: A number datatype that contains the number of albumin for each person
10. **Albumin_and_Globulin_Ratio**: A number datatype that contains the Albumin and Globulin Ratio for each person
11. **Dataset**: An integer datatype which consist of 1 for liver patients and 2 for non-liver patients.

**First 10 Rows of pdata dataset**

```{r,echo=FALSE}
pdata %>% head(n=10)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
```


\newpage 

## Data Cleaning

Looking at the first six rows of the pdata data frame, the data needs additional work for the data to be classified as clean data. There are four NAs in the Albumin and Globulin ratio. Since it is a ration, we can change the NAs to the average of the ratio. To really work on the dataset, we would also need to make changes to the Dataset variable. This is because it would be easier to calculate if we use 0 to represent not being a liver patient instead of using 2. Fixing these three problems will result in the data becoming clean.

Therefore, for the pre-processing phase, the following steps will be taken:

1. Replace the NAs in the Albumin and Globulin ratio with 0s

```{r}
 #1.	Replace the NAs in the Albumin and Globulin ratio with 0s

        pdata$Albumin_and_Globulin_Ratio[is.na(pdata$Albumin_and_Globulin_Ratio)]<- 
                    mean(pdata$Albumin_and_Globulin_Ratio,na.rm = TRUE)

        sapply(pdata, function(g){
          sum(is.na(g))
        })%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```
  
2. Replacing the 2s in the Dataset column with 0s

```{r}
#2.	Replacing the 2s in the Dataset column with 0s
        pdata<- pdata%>%mutate(Dataset= ifelse(Dataset ==2,0,1))
        pdata%>%group_by(Dataset)%>%summarise(count=n())%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
```

3. Create a column for Indirect Bilirubin 

```{r}
#3.	Create a column for Indirect Bilirubin 
  pdata<- pdata%>%mutate(Indirect_Bilirubin= 
                                  Total_Bilirubin - Direct_Bilirubin )
        
    #Changing the order of the dataset
    pdata<- pdata%>%select(Age,Gender,Total_Bilirubin, 
                               Direct_Bilirubin,Indirect_Bilirubin,
                               Alkaline_Phosphotase, Alamine_Aminotransferase,
                               Aspartate_Aminotransferase,Total_Protiens,
                               Albumin,Albumin_and_Globulin_Ratio, Dataset)
 pdata%>%head(n=3)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
```

4. Change Dataset variable name to Disease_Status

```{r}
pdata<- pdata%>%mutate(Disease_Status=Dataset)%>%select(-Dataset)
pdata%>%head(n=3)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
```




**First 10 rows in Cleaned pdata dataset**

```{r, echo=FALSE}
 pdata %>% head(n=10)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
```

\newpage 

## Visualization 

We will start off by taking a look at the level of disparity between males and females in the data. 

### Frequency between Male and Female

This shows that is really a great level of disparity between the amount of males and females inside the dataset.

```{r, echo=FALSE}
    
    #Frequency between Male and Female
        pdata %>%
          ggplot(aes(Gender, fill= Gender)) +
          theme_minimal()  +
          geom_bar() +
          scale_x_discrete() +
          scale_y_continuous(labels = scales::comma) +
          labs(title = "Frequency between Male and Female",
               x = "Gender",
               y = "Frequency")
        
```

     
### Liver Patient Status vs Density

This graph shows us that more than half of the persons in the dataset have liver disease.

```{r, echo=FALSE}
#Liver Patient Status vs Density
        pdata%>%ggplot( aes(Disease_Status)) + 
          geom_histogram(aes(y = ..density..),binwidth = 1, colour = "black", fill = "white")+
        geom_density(alpha = 0.2, fill = "#FF6666") + ggtitle("Liver Patient Status vs Density")

#Table showing the distribution of liver patients by Gender
        pdata %>%group_by(Gender)%>%summarise(Num_Liver_Patients = sum(Disease_Status))%>%
        kable() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
      
```


### Visualizing all the numeric data in the dataset

```{r, echo=FALSE}
 # Plotting all numeric data
        plot_num(pdata %>% select(-Gender), bins=10) 
```


### Checking the correlation between the chemicals

Machine learning algorithms suggest that all predictor variable shourl be independent of each other. From this correlation visualization one may suggest that there is not much correlation between the chemicals. However, there is some level of correlation between Total Bilirubin and Indirect Bilirubin. 

```{r, echo=FALSE}
# Check the correlation between chemicals 
        correlationMatrix <- cor(pdata[,3:ncol(pdata)])
        corrplot(correlationMatrix, type = "upper", order = "hclust", 
                 tl.col = "black",tl.cex = 1, addrect = 8) 
```

### Correlation between Total Bilirubin and Indirect Bilirubin

There is a high level of correlation between the two parameter. Therefore to get the best out of our model, we will have to remove one. In the Caret package, there is a function called *"findCorrelation"* that will determine which parameter to take out of our model. With a cutoff at 90%, the fuction determined that Total Bilirubin should be the parameter taken out.

```{r, echo=FALSE}
#Correlation between Total_Bilirubin and Indirect_Bilirubin
        ggplot(pdata, aes(x=Total_Bilirubin, y=Indirect_Bilirubin, fill=Disease_Status)) +
          geom_point(size=2, shape=23)
```

\newpage 

# Modeling Approach


## Principal Component Analysis (PCA)

This is a dimensionality-reduction technique that is used to explain the variance-covariance structure of a set of variables through linear combinations. The main reason for using the PCA is to reduce the amount of memory and computation power used since we would have less variables. 

We will use the function *‘prncomp’* to calculate the PCA so that we can prevent redundancy and relavancy.

```{r}
#Calculating and plotting PCA
pca_pdata <- prcomp(pdata[,3:ncol(pdata)], center = TRUE, scale = TRUE)
        plot(pca_pdata, type="l")
```

```{r}
#Finding the level of Importance for each paramter
summary(pca_pdata)
```

Now, let use the decision that we got from *findCorrelation* to optimize the variables needed for our model.

```{r}
#Removing Total Biliribin from the model
        pdata<-pdata%>%select(-Total_Bilirubin)
        
    #Calculating and plotting PCA
        pca_pdata <- prcomp(pdata[,3:ncol(pdata)], center = TRUE, scale = TRUE)
        plot(pca_pdata, type="l")
        
    #Finding the level of Importance for each paramter
        summary(pca_pdata)
```

As such, to get the best model, we should in deep remove the Total Biliribin parameter.

## Creation of Training and Test Set

Before we can start modelling our data, we  first have to decide on which cross validation technique to choose.
*Cross-validation* refers to a set of methods for measuring the performance of a given predictive model on new test data sets. 

For this project, we have chosen the 10-fold cross-validation as our cross-validation method for assessing model performance. Ihave chosen this model because the dataset is not that huge, therefore the 10-fold cross-validation should be less bias than the other cross-validation methods. To use this method, however, we will set the seed to 1 for reproductivity.

We first have to split our data into a training and a test set. The the Caret package contains the fuction *"createDataPartition"* that will do the splitting for us. Here, we have chosen to split our data, with 20% going to the test set and 80% going to the training set. This ratio was chosen becase would like to have as much of our data to train while having our test set being large enough to obtain stable estimates of the loss.

Lets start by converting our outcome variable Disease_Status to a 2 factor variable

```{r}
# Setting outcome as a 2 level factor variable which changing format to use ROC
pdata<- pdata%>%mutate(Disease_Status= 
                                 ifelse(Disease_Status ==1,"yes","no"))
        pdata$Disease_Status <- as.factor(pdata$Disease_Status)
```


```{r}
 # Split the dataset into train and test set
   # Set seed as a starting point
        set.seed(1, sample.kind='Rounding')

        train_index <- createDataPartition(
          y = pdata$Disease_Status, 
          p = 0.2, 
          list = F
        )
        training_set <- pdata[-train_index,]
        test_set <- pdata[train_index,]
```

```{r}
#The dimension of the training set
        dim(training_set)
```

```{r}
#The dimension of the test set      
        dim(test_set)
```


After the splitting of the data we will create a function *"train.control"* to allow us set train controll as  10-fold cross-validation. Finally, we will then train our algorithm on the training set and use the test set to predict and compute our apparent errors. 

```{r}
# Define training control
      #method = cv -> Cross Validation method
      #number = 10 -> The number of folds
      # Set seed as a starting point
        
        set.seed(1, sample.kind='Rounding')
        train.control <- trainControl(method = "cv",
                                      number = 10,
                                      classProbs = TRUE,
                                      summaryFunction = twoClassSummary)
```


```{r}
 head(test_set)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
        
```

```{r}
 head(training_set)%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = TRUE)
        
```

## Logistic Regression Model (GLM)

We will start off our modeling with GLM. Firstly, Logical Regression is seen as one of the specified cases of the general linear model. This model takes categoric variables as its outcome, which is similar to what we have in our dataset, 1 for liver patient and 0 for non liver patient. As such we will use the function *"glm"* to fit our model. However in order to use this function, we would have to use the family function to specify the version of this model that we would like to use. 

```{r message=FALSE}
set.seed(1, sample.kind='Rounding')
        
        glm_fit<- train(Disease_Status ~., data = training_set , method = "glm",
                             metric = "ROC",
                             preProcess = c("scale", "center"),  # in order to normalize the data
                             trControl= train.control,
                             family = "binomial")
        
        # Predciting on the test data
        glm_pred<- predict( glm_fit, test_set)
        
        # Calculating the confusion matrix
        glm_conf_matrix <- confusionMatrix(glm_pred, test_set$Disease_Status, positive = "yes")
        
        #Storing the Accuracy and sensitivity level for glm model
         glm_Accuracy <- glm_conf_matrix$overall[['Accuracy']]
         glm_Sensitivity <- sensitivity(glm_pred, test_set$Disease_Status, positive="yes")
        
        
        #Calculating F1 Score
        
        precision <- posPredValue(glm_pred, test_set$Disease_Status, positive="yes")
        recall <- sensitivity(glm_pred, test_set$Disease_Status, positive="yes")
        
        glm_F1_Score <- (2 * precision * recall) / (precision + recall)
        
        
        # Creating a data frame to store the results from our models
        model_results <- data.frame(model="Logistic Regression Model", 
                                    Accuracy =glm_Accuracy,
                                    Sensitivity = glm_Sensitivity ,
                                    F1_Score = glm_F1_Score)
        
        table("Accuracy" = glm_Accuracy, "Sensitivity" = glm_Sensitivity, 
              "F1_Score" = glm_F1_Score  )%>%
              kable() %>%
              kable_styling(bootstrap_options = c("striped", "hover", 
                                                  "condensed", "responsive"),
                                                position = "center",
                                                font_size = 10,
                                                full_width = FALSE)
```


## K-nearest neighbors (kNN) Model

This algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. As such, it relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data. However, it is easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.


```{r message=FALSE}
set.seed(1, sample.kind='Rounding')
        
        knn_fit<- train(Disease_Status ~., data = training_set , method = "knn",
                        metric = "ROC",
                        preProcess = c("scale", "center"),  # in order to normalize the data
                        trControl= train.control,
                        tuneLength=10)
        
        # Predciting on the test data
        knn_pred<- predict( knn_fit, test_set)
        
        # Calculating the confusion matrix
        knn_conf_matrix <- confusionMatrix(knn_pred, test_set$Disease_Status, positive = "yes")
        
        #Storing the Accuracy and sensitivity level for knn model
        knn_Accuracy <- knn_conf_matrix$overall[['Accuracy']]
        knn_Sensitivity <- sensitivity(knn_pred, test_set$Disease_Status, positive="yes")
        
        
        #Calculating F1 Score
        precision <- posPredValue(knn_pred, test_set$Disease_Status, positive="yes")
        recall <- sensitivity(knn_pred, test_set$Disease_Status, positive="yes")
        
        knn_F1_Score <- (2 * precision * recall) / (precision + recall)
        
        #Adding the result to the data frame created earlier
        model_results <- model_results %>% add_row(model="K-Nearest Neighbors Model",
                                                   Accuracy =knn_Accuracy,
                                                   Sensitivity = knn_Sensitivity ,
                                                   F1_Score = knn_F1_Score)
        
        table("Accuracy" = knn_Accuracy, "Sensitivity" = knn_Sensitivity, 
              "F1_Score" = knn_F1_Score  )%>%
              kable() %>%
              kable_styling(bootstrap_options = c("striped", "hover", 
                                                  "condensed", "responsive"),
                                                position = "center",
                                                font_size = 10,
                                                full_width = FALSE)
```



## Rain Forrest Model

This is a renowed machine learning approach that solves the errors from the decision tree. Rain Forrest is an algorithm of constructing a decision tree (how to do splitting) when the dataset is so large that it does not fit the memory. In rain forest, the whole dataset is not required for making a splitting decision. Only some aggregated information (AVC-set for an attribute or AVC-group if you have more memory) is required.

```{r message=FALSE}
set.seed(1, sample.kind='Rounding')
        
        rf_fit<- train(Disease_Status ~., data = training_set , method = "rf",
                        metric = "ROC",
                        preProcess = c("scale", "center"),  # in order to normalize the data
                        trControl= train.control,
                        tuneLength=10)
        
        # Predciting on the test data
        rf_pred<- predict( rf_fit, test_set)
        
        # Calculating the confusion matrix
        rf_conf_matrix <- confusionMatrix(rf_pred, test_set$Disease_Status, positive = "yes")
        
        #Storing the Accuracy and sensitivity level for knn model
        rf_Accuracy <- rf_conf_matrix$overall[['Accuracy']]
        rf_Sensitivity <- sensitivity(rf_pred, test_set$Disease_Status, positive="yes")
        
        
        #Calculating F1 Score
        
        precision <- posPredValue(rf_pred, test_set$Disease_Status, positive="yes")
        recall <- sensitivity(rf_pred, test_set$Disease_Status, positive="yes")
        
        rf_F1_Score <- (2 * precision * recall) / (precision + recall)
        
        #Adding the result to the data frame created earlier
        model_results <- model_results %>% add_row(model="Random Forrest Model",
                                                   Accuracy =rf_Accuracy,
                                                   Sensitivity = rf_Sensitivity ,
                                                   F1_Score = rf_F1_Score)
        
        table("Accuracy" = rf_Accuracy, "Sensitivity" = rf_Sensitivity, 
              "F1_Score" = rf_F1_Score  )%>%
              kable() %>%
              kable_styling(bootstrap_options = c("striped", "hover", 
                                                  "condensed", "responsive"),
                                                position = "center",
                                                font_size = 10,
                                                full_width = FALSE)
       
        
```


## Support Vector Machines Algorithm

This is a supervised learning method that looks at data and sorts it into one of two categories. An SVM outputs a map of the sorted data with the margins between the two as far apart as possible. In fact, this machine algorithm can be used to analyzes data for classification and regression analysis. 

```{r message=FALSE}
set.seed(1, sample.kind='Rounding')
        
        svm_fit<- svm(Disease_Status~., data = training_set, kernel = "linear")
        svm_fit
        summary(svm_fit)
        
        svm_pred = predict(svm_fit,test_set)
        svm_conf_matrix <- confusionMatrix(reference = 
                                      as.factor(test_set$Disease_Status), data = svm_pred)
      
        svm_Accuracy <- svm_conf_matrix$overall[['Accuracy']]
        svm_Sensitivity <- sensitivity(svm_pred, test_set$Disease_Status, positive="yes")
        
        
        #Calculating F1 Score
        
        precision <- posPredValue(svm_pred, test_set$Disease_Status, positive="yes")
        recall <- sensitivity(svm_pred, test_set$Disease_Status, positive="yes")
        
        svm_F1_Score <- (2 * precision * recall) / (precision + recall)
        
        #Adding the result to the data frame created earlier
        model_results <- model_results %>% add_row(model="Support Vector Model",
                                                   Accuracy =svm_Accuracy,
                                                   Sensitivity = svm_Sensitivity ,
                                                   F1_Score = svm_F1_Score)
        #Table of results
        table("Accuracy" = svm_Accuracy, "Sensitivity" = svm_Sensitivity, 
              "F1_Score" = svm_F1_Score)%>%
              kable() %>%
              kable_styling(bootstrap_options = c("striped", "hover", 
                                                  "condensed", "responsive"),
                                                position = "center",
                                                font_size = 10,
                                                full_width = FALSE)
        
```



\newpage 

# Results

This is the summary results for all the model built, that were trained on trainig set dataset and validated on the test set dataset.

```{r}
model_results%>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```

In order to evaluate how good the predictions made by that model are, we first have to look at three matrics of our models.

1. Accuracy - We will look at this aspect because it it is the measure of all the correctly identified cases.  In order for the doctors to rely on this algorithm, it has to be trust worthy.
    
$$\mbox{Accuracy} = \frac{True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}$$

2. Sensitivity - We will also look at sensitivity because it shows how many times the algorithm diagnosed a person with the liver disease that actually had the disease. Therefore the most important aspect is to make shore that the algorithm gives the correct verdict.

   $$\mbox{Sensitivity} = \frac{True Positive}{True Positive + False Negative}$$
   
   
3. F1 Score - The last matric we will look at is the F1 score. This is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy Metric.Here precision is the measure of the correctly identified positive cases from all the predicted positive cases while Recall is the measure of the correctly identified positive cases from all the actual positive cases.
 
   $$\mbox{Recall} = \frac{True Positive}{True Positive + False Negative}$$
   
   $$\mbox{Precision} = \frac{True Positive}{True Positive + False Positive}$$
 
   $$\mbox{F1 Score} = 2 *{\frac{(Precision * Recall)}{(Precision + Recall)}}$$
   
Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial. 

Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.

Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.

However, the it is quite easy to select the model that best optimizes the model, since the rain forrest model had the highest result for both Accuracy and F1 Score. In addition, approximately 92% percent of the persons with liver disease were correctly classified.

Even though the support vector had the 100% percent of the persons with liver disease were correctly classified, it still wasnot as precise and accurate as the rain forrest model.

Both the Logistic Regression model and the K-Nearest Neighbors Model had the same level of sensitivity. However, the Logical Regression Model had the higher accuracy and F1 score.

Therefore, it is fair to say that the worst model was the K-Nearest Neighbors Model while the best model was the Logistic Regression Model.

\newpage 

# Conclusion

For this project, we investigated several machine learning models and we selected the optimal model by observing the accuracy, sensitivity along with the F1 Score of each model. The verdict of our model was that the worst model was the K-Nearest Neighbors Model while the best model was the Logistic Regression Model.